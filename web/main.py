# app.py
# -*- coding: utf-8 -*-

import io
import math
import re
import zipfile
from pathlib import Path

import networkx as nx
import pandas as pd
import streamlit as st
from pyvis.network import Network
import streamlit.components.v1 as components


st.set_page_config(
    page_title="Daiyu Character Network Study",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown(
    """
<style>
.block-container { padding-top: 1.5rem; }
h1, h2, h3 { letter-spacing: .3px; }
.card {
    background: #0f172a10;
    border: 1px solid #e2e8f0;
    padding: 14px 16px;
    border-radius: 14px;
    margin-bottom: 10px;
}
.small { color: #64748b; font-size: 0.9rem; }
hr { margin: 1.2rem 0; }
</style>
""",
    unsafe_allow_html=True,
)

st.title("Hong Lou Meng: Daiyu Interaction Network")
st.caption(
    "Co-occurrence / Dialogue / Sentiment Trend analysis based on Chapters 10–20 (Streamlit presentation)"
)

st.markdown(
    """
<div class="card">
<b>Research question</b>: Which characters interact most frequently with Daiyu in the early narrative, and does the network suggest layered social or emotional structure?
<br><span class="small">Upload your analysis CSV/TXT files to generate the network, summary statistics, and sentiment trends.</span>
</div>
""",
    unsafe_allow_html=True,
)


st.sidebar.header("Data Files")
st.sidebar.markdown(
    "Upload your analysis outputs here for visualization and cross-checking."
)

edge_file = st.sidebar.file_uploader(
    "Co-occurrence edges CSV (Source, Target, Weight) — required",
    type=["csv"],
)
node_file = st.sidebar.file_uploader(
    "Node attributes CSV (optional: gephi_nodes.csv)",
    type=["csv"],
)
sent_file = st.sidebar.file_uploader(
    "Sentence results TXT (optional: daiyu_interaction_sentences.txt)",
    type=["txt"],
)

output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

st.sidebar.markdown("---")
st.sidebar.subheader("Download Current Outputs (output.zip)")


def zip_output_folder(folder: Path) -> bytes:
    mem = io.BytesIO()
    with zipfile.ZipFile(mem, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in folder.glob("*"):
            if p.is_file():
                zf.writestr(p.name, p.read_bytes())
    mem.seek(0)
    return mem.read()


zip_bytes = zip_output_folder(output_dir)
st.sidebar.download_button(
    "Download output.zip",
    data=zip_bytes,
    file_name="output.zip",
    mime="application/zip",
    help="Includes nodes/edges tables, GEXF, and derived stats generated by this app.",
)


def load_csv_or_default(uploaded, default_name):
    if uploaded is not None:
        return pd.read_csv(uploaded)
    p = Path(default_name)
    if p.exists():
        return pd.read_csv(p)
    return None


def load_txt_or_default(uploaded, default_name):
    if uploaded is not None:
        return uploaded.read().decode("utf-8", errors="ignore")
    p = Path(default_name)
    if p.exists():
        return p.read_text(encoding="utf-8", errors="ignore")
    return None


edges = load_csv_or_default(edge_file, "daiyu_edges_cooccur.csv")
nodes = load_csv_or_default(node_file, "gephi_nodes.csv")
sent_txt = load_txt_or_default(sent_file, "daiyu_interaction_sentences.txt")

if edges is None:
    st.warning(
        "Please upload daiyu_edges_cooccur.csv (or place it in the same folder as app.py)."
    )
    st.stop()


G = nx.from_pandas_edgelist(
    edges,
    source="Source",
    target="Target",
    edge_attr="Weight",
    create_using=nx.Graph(),
)

n_nodes = G.number_of_nodes()
n_edges = G.number_of_edges()
density = nx.density(G)

deg = dict(G.degree())
strength = dict(G.degree(weight="Weight"))

if nodes is None or "Id" not in nodes.columns:
    bet = nx.betweenness_centrality(G, weight="Weight", normalized=True)
    clo = nx.closeness_centrality(G)
    eig = nx.eigenvector_centrality(G, weight="Weight", max_iter=1000)

    nodes = pd.DataFrame(
        {
            "Id": list(G.nodes()),
            "Label": list(G.nodes()),
            "degree": [deg[n] for n in G.nodes()],
            "strength": [strength[n] for n in G.nodes()],
            "betweenness": [bet[n] for n in G.nodes()],
            "closeness": [clo[n] for n in G.nodes()],
            "eigenvector": [eig[n] for n in G.nodes()],
        }
    )

nodes_path = output_dir / "gephi_nodes.csv"
edges_path = output_dir / "gephi_edges.csv"
gexf_path = output_dir / "daiyu_network.gexf"
nodes.to_csv(nodes_path, index=False, encoding="utf-8")
edges.to_csv(edges_path, index=False, encoding="utf-8")
nx.write_gexf(G, gexf_path)


def layer_daiyu_by_strength(nodes_df: pd.DataFrame):
    df = nodes_df.copy()
    df = df[df["Id"] != "黛玉"].sort_values("strength", ascending=False)
    if len(df) == 0:
        return {"High-frequency": [], "Mid-frequency": [], "Low-frequency": []}

    q_high = df["strength"].quantile(0.67)
    q_mid = df["strength"].quantile(0.33)

    high = df[df["strength"] >= q_high]["Id"].tolist()
    mid = df[(df["strength"] < q_high) & (df["strength"] >= q_mid)]["Id"].tolist()
    low = df[df["strength"] < q_mid]["Id"].tolist()

    return {"High-frequency": high, "Mid-frequency": mid, "Low-frequency": low}


layers = layer_daiyu_by_strength(nodes)


def auto_network_interpretation(layers_dict, edges_df):
    top_edges = edges_df.sort_values("Weight", ascending=False).head(5)
    lines_out = []
    lines_out.append("**Network interpretation**")
    lines_out.append(
        f"- Daiyu ego-network: {n_nodes} nodes, {n_edges} edges, density {density:.3f}."
    )
    if len(top_edges) > 0:
        first = top_edges.iloc[0]
        lines_out.append(
            f"- Strongest tie: **{first['Source']}—{first['Target']} ({first['Weight']})**."
        )
    lines_out.append(
        f"- High-frequency interactors: {', '.join(layers_dict['High-frequency']) if layers_dict['High-frequency'] else '(none)'}"
    )
    lines_out.append(
        f"- Mid-frequency interactors: {', '.join(layers_dict['Mid-frequency']) if layers_dict['Mid-frequency'] else '(none)'}"
    )
    lines_out.append(
        f"- Low-frequency interactors: {', '.join(layers_dict['Low-frequency']) if layers_dict['Low-frequency'] else '(none)'}"
    )
    return "\n".join(lines_out)


def parse_sentiments(text: str):
    rows = []
    if not text:
        return pd.DataFrame(columns=["idx", "sentiment", "section", "line"])

    current_section = "Unknown"
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue

        sec_m = re.match(r"【(.+?)】", line)
        if sec_m:
            current_section = sec_m.group(1)
            continue

        m = re.match(r"\[句(\d+)\].*?情感:([^\s|]+)", line)
        if m:
            idx = int(m.group(1))
            sentiment = m.group(2)
            rows.append(
                {
                    "idx": idx,
                    "sentiment": sentiment,
                    "section": current_section,
                    "line": line,
                }
            )
    return pd.DataFrame(rows)


def sentiment_trend(df: pd.DataFrame, bin_size=150):
    if df is None or df.empty:
        return pd.DataFrame(columns=["bin", "正面", "负面", "中性", "混合"])
    if "idx" not in df.columns or "sentiment" not in df.columns:
        return pd.DataFrame(columns=["bin", "正面", "负面", "中性", "混合"])

    df = df.copy()
    df["idx"] = pd.to_numeric(df["idx"], errors="coerce")
    df = df.dropna(subset=["idx"])
    df["bin"] = (df["idx"].astype(int) // bin_size) * bin_size

    pivot = df.pivot_table(
        index="bin", columns="sentiment", aggfunc="size", fill_value=0
    )
    for col in ["正面", "负面", "中性", "混合"]:
        if col not in pivot.columns:
            pivot[col] = 0
    pivot = pivot[["正面", "负面", "中性", "混合"]].reset_index()
    return pivot


sent_df = (
    parse_sentiments(sent_txt)
    if sent_txt
    else pd.DataFrame(columns=["idx", "sentiment", "section", "line"])
)
trend_df = sentiment_trend(sent_df)


tab1, tab2, tab3, tab4 = st.tabs(
    ["Overview", "Network", "Layers & Findings", "Sentiment Trend"]
)


with tab1:
    c1, c2, c3 = st.columns(3)
    c1.metric("Nodes", n_nodes)
    c2.metric("Edges", n_edges)
    c3.metric("Density", f"{density:.3f}")

    st.markdown("#### Top 10 strongest ties (by weight)")
    st.dataframe(
        edges.sort_values("Weight", ascending=False).head(10),
        use_container_width=True,
    )

    st.markdown("#### Top 10 nodes by strength (weighted degree)")
    st.dataframe(
        nodes.sort_values("strength", ascending=False).head(10)[
            ["Id", "strength", "degree", "betweenness", "eigenvector"]
        ],
        use_container_width=True,
    )


with tab2:
    st.markdown("### Interactive Daiyu ego-network")
    st.caption(
        "Colors indicate interaction frequency tiers; node size reflects strength; edge width reflects weight."
    )

    net = Network(
        height="760px",
        width="100%",
        bgcolor="#ffffff",
        font_color="#0f172a",
        directed=False,
    )

    nodes_no_daiyu = nodes[nodes["Id"] != "黛玉"].copy()
    q_high = nodes_no_daiyu["strength"].quantile(0.67)
    q_mid = nodes_no_daiyu["strength"].quantile(0.33)

    def color_by_strength(name):
        if name == "黛玉":
            return "#111827"
        if name == "寶玉":
            return "#e11d48"
        s = strength.get(name, 1)
        if s >= q_high:
            return "#ff4d6d"
        elif s >= q_mid:
            return "#ffb42a"
        else:
            return "#60a5fa"

    ordered = nodes_no_daiyu.sort_values("strength", ascending=False)["Id"].tolist()
    high = [n for n in ordered if strength[n] >= q_high]
    mid = [n for n in ordered if q_mid <= strength[n] < q_high]
    low = [n for n in ordered if strength[n] < q_mid]

    R_HIGH, R_MID, R_LOW = 160, 290, 420

    def ring_positions(names, R):
        pos_local = {}
        if not names:
            return pos_local
        step = 2 * math.pi / len(names)
        for i, n in enumerate(names):
            ang = i * step
            pos_local[n] = (R * math.cos(ang), R * math.sin(ang))
        return pos_local

    pos = {}
    pos.update(ring_positions(high, R_HIGH))
    pos.update(ring_positions(mid, R_MID))
    pos.update(ring_positions(low, R_LOW))
    pos["黛玉"] = (0, 0)

    for n in ["黛玉"] + ordered:
        s = strength.get(n, 1)
        size = 12 + (s**0.5) * 2.2
        font_size = 18 + (s**0.25) * 5
        x, y = pos.get(n, (0, 0))

        net.add_node(
            n,
            label=n,
            x=x,
            y=y,
            physics=True,
            fixed=False,
            size=size,
            color=color_by_strength(n),
            borderWidth=2,
            font={"size": font_size, "face": "Microsoft YaHei"},
        )

    for u, v, d in G.edges(data=True):
        w = float(d.get("Weight", 1))
        net.add_edge(
            u,
            v,
            value=w,
            width=0.8 + (w**0.6) / 5,
            color="#94a3b8",
        )

    net.set_options(
        """
{
  "physics": {
    "enabled": true,
    "solver": "forceAtlas2Based",
    "forceAtlas2Based": {
      "gravity": -6,
      "centralGravity": 0.02,
      "springLength": 210,
      "springConstant": 0.02,
      "avoidOverlap": 1.3
    },
    "minVelocity": 0.8,
    "stabilization": {
      "enabled": true,
      "iterations": 400,
      "updateInterval": 25
    }
  },
  "interaction": {
    "dragNodes": true,
    "zoomView": true,
    "dragView": true
  }
}
"""
    )

    net.save_graph(str(output_dir / "circle_draggable_net.html"))
    html = (output_dir / "circle_draggable_net.html").read_text(encoding="utf-8")
    components.html(html, height=780, scrolling=True)

    st.download_button(
        "Download Gephi file (GEXF)",
        data=gexf_path.read_bytes(),
        file_name="daiyu_network.gexf",
        mime="application/octet-stream",
    )


with tab3:
    st.markdown("### Interaction frequency tiers (by strength)")

    colL, colR = st.columns([1.1, 1.5])

    with colL:
        st.markdown("#### Tier membership")
        st.markdown(
            f"**High-frequency**: {', '.join(layers['High-frequency']) if layers['High-frequency'] else '(none)'}"
        )
        st.markdown(
            f"**Mid-frequency**: {', '.join(layers['Mid-frequency']) if layers['Mid-frequency'] else '(none)'}"
        )
        st.markdown(
            f"**Low-frequency**: {', '.join(layers['Low-frequency']) if layers['Low-frequency'] else '(none)'}"
        )
        st.info(
            "Tiers are computed by strength quantiles: top third / middle third / bottom third."
        )

    with colR:
        st.markdown("#### Strength distribution")
        show_df = nodes[nodes["Id"] != "黛玉"].sort_values("strength", ascending=False)
        st.bar_chart(show_df.set_index("Id")["strength"])

    st.markdown("---")
    st.markdown(auto_network_interpretation(layers, edges))


with tab4:
    st.markdown("### Sentiment trend over the text")
    st.caption(
        "Sentence indices are used as a narrative timeline; counts are aggregated in bins."
    )

    if trend_df.empty:
        st.warning(
            "No sentiment lines detected. Upload daiyu_interaction_sentences.txt to enable this panel."
        )
    else:
        st.line_chart(trend_df.set_index("bin")[["正面", "负面", "中性", "混合"]])

        st.markdown("#### Overall sentiment counts")
        total_counts = sent_df["sentiment"].value_counts()
        st.dataframe(
            total_counts.rename_axis("sentiment").reset_index(name="count"),
            use_container_width=True,
        )

        st.markdown("#### Sentence finder")

        senti_choice = st.selectbox(
            "Sentiment filter", ["All", "正面", "负面", "中性", "混合"]
        )
        q = st.text_input("Keyword filter (e.g., 寶玉 / 鳳姐)", "")
        sec_choice = st.selectbox(
            "Section filter",
            ["All"] + sorted(sent_df["section"].unique().tolist()),
        )

        df_show = sent_df.copy()
        if senti_choice != "All":
            df_show = df_show[df_show["sentiment"] == senti_choice]
        if sec_choice != "All":
            df_show = df_show[df_show["section"] == sec_choice]
        if q:
            df_show = df_show[df_show["line"].str.contains(q, na=False)]

        df_show = df_show.sort_values("idx")

        st.write(f"Matched sentences: {len(df_show)}")

        page = st.number_input("Page (25 per page)", min_value=1, value=1)
        per_page = 25
        start = (page - 1) * per_page
        end = start + per_page

        for line in df_show["line"].iloc[start:end].tolist():
            st.write(line)

        st.download_button(
            "Download filtered sentences",
            data="\n".join(df_show["line"].tolist()).encode("utf-8"),
            file_name="filtered_sentences.txt",
            mime="text/plain",
        )


st.sidebar.markdown("---")
st.sidebar.caption("Shen Ziqi 25054114g")
st.sidebar.caption("Assignment 2")
